# PANDAS + KERAS CHEATSHEET FOR REAL ESTATE ML

============================================================
PANDAS - DATA MANIPULATION
============================================================

LOADING DATA:
    import pandas as pd
    df = pd.read_csv('file.csv')

EXPLORING DATA:
    df.head()              # First 5 rows
    df.shape               # (rows, columns)
    df.columns             # Column names
    df.dtypes              # Data types
    df.info()              # Summary info
    df.describe()          # Statistics
    df.isnull().sum()      # Missing values

SELECTING DATA:
    df['price']                           # Single column
    df[['price', 'sqft']]                 # Multiple columns
    df[df['price'] > 500000]              # Filter rows
    df[(df['bedrooms'] >= 3) & (df['price'] < 400000)]

DATA MANIPULATION:
    df['price_per_sqft'] = df['price'] / df['sqft']   # New column
    df.drop('property_id', axis=1)                     # Drop column
    df.sort_values('price', ascending=False)           # Sort
    df.groupby('location')['price'].mean()             # Group by

ENCODING CATEGORICAL VARIABLES:
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    df['location_encoded'] = le.fit_transform(df['location'])

============================================================
KERAS - DEEP LEARNING
============================================================

BUILDING A MODEL:
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout

    model = Sequential([
        Dense(64, activation='relu', input_shape=(n_features,)),
        Dropout(0.2),
        Dense(32, activation='relu'),
        Dense(16, activation='relu'),
        Dense(1)  # Output: 1 for regression
    ])

COMPILING THE MODEL:
    # For Regression
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    
    # For Classification
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

TRAINING:
    from tensorflow.keras.callbacks import EarlyStopping
    
    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    
    history = model.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=100,
        batch_size=32,
        callbacks=[early_stop]
    )

EVALUATE & PREDICT:
    loss, mae = model.evaluate(X_test, y_test)
    predictions = model.predict(X_new)

SAVE & LOAD:
    model.save('my_model.keras')
    
    from tensorflow.keras.models import load_model
    model = load_model('my_model.keras')

============================================================
PREPROCESSING (SKLEARN)
============================================================

TRAIN-TEST SPLIT:
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

FEATURE SCALING (CRITICAL FOR NEURAL NETWORKS!):
    from sklearn.preprocessing import StandardScaler
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)  # Use .transform(), not .fit_transform()!

============================================================
COMPLETE WORKFLOW
============================================================

# 1. Load data
df = pd.read_csv('real_estate.csv')

# 2. Clean data
df = df.drop('id', axis=1)

# 3. Encode categoricals
le = LabelEncoder()
df['location'] = le.fit_transform(df['location'])

# 4. Split features and target
X = df.drop('price', axis=1)
y = df['price']

# 5. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 6. Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 7. Build model
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# 8. Train
history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100)

# 9. Evaluate
test_loss, test_mae = model.evaluate(X_test_scaled, y_test)

# 10. Predict
predictions = model.predict(X_test_scaled)

============================================================
TIPS & BEST PRACTICES
============================================================

PANDAS:
  - Always check for missing values first
  - Remove irrelevant columns (IDs, etc.)
  - Encode categoricals before modeling
  - Use df.copy() to avoid modifying original

KERAS:
  - ALWAYS scale features before training
  - Start simple, add complexity later
  - Use EarlyStopping to prevent overfitting
  - Plot training history to diagnose problems
  - Save your model after training

COMMON MISTAKES:
  - Forgetting to scale features
  - Using .fit_transform() on test data (use .transform())
  - Not using dropout for regularization
  - Using wrong loss function for task type
